import json
import os
import re
import json_repair
from pathlib import Path
from typing import Dict, Any, List, Optional
from logger import system_logger
import db
import glossary_manager

def collect_and_deduplicate_terms(workspace_paths: Dict[str, Any]) -> Dict[str, Any]:
    terms_dir = workspace_paths.get("terms")
    if not terms_dir or not os.path.exists(terms_dir):
        return {}
    unique_terms = {}
    for filename in os.listdir(terms_dir):
        if not filename.endswith(".json"): continue
        file_path = os.path.join(terms_dir, filename)
        try:
            with open(file_path, 'r', encoding='utf-8') as f: cli_output = json.load(f)
            if not (isinstance(cli_output, dict) and "response" in cli_output): continue
            response_str = cli_output["response"]
            match = re.search(r'```json\s*\n(.*?)\s*\n```', response_str, re.DOTALL)
            json_str = match.group(1) if match else response_str
            data = json_repair.loads(json_str)
            if not isinstance(data, dict): continue
            for category, items in data.items():
                if not isinstance(items, dict): continue
                for term_id, term_data in items.items():
                    if term_id not in unique_terms:
                        unique_terms[term_id] = {"category": category, "data": term_data}
        except (json.JSONDecodeError, IOError, TypeError) as e:
            system_logger.error(f"[TermCollector] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ '{filename}': {e}")
    final_structure = {"characters": {}, "terminology": {}, "expressions": {}}
    for term_id, term_info in unique_terms.items():
        cat = term_info["category"]
        if cat in final_structure:
            final_structure[cat][term_id] = term_info["data"]
    return final_structure

def _edit_term(term_data: Dict[str, Any]) -> Dict[str, Any]:
    system_logger.info("\n--- –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–∞ ---")
    for key in ["ru", "jp", "romaji"]:
        new_val = input(f"  name.{key} (Enter, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å '{term_data['name'].get(key, '')}'): ").strip()
        if new_val: term_data['name'][key] = new_val
    new_desc = input(f"  description (Enter, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å '{term_data.get('description', '')}'): ").strip()
    if new_desc: term_data['description'] = new_desc
    new_context = input(f"  context (Enter, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å '{term_data.get('context', '')}'): ").strip()
    if new_context: term_data['context'] = new_context
    system_logger.info(f"  –¢–µ–∫—É—â–∏–µ –ø—Å–µ–≤–¥–æ–Ω–∏–º—ã: {[a.get('ru', '') for a in term_data.get('aliases', [])]}")
    if input("  –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Å–µ–≤–¥–æ–Ω–∏–º—ã? (y/n): ").lower() == 'y':
        term_data['aliases'] = []
        while True:
            alias_ru = input("    –î–æ–±–∞–≤–∏—Ç—å –ø—Å–µ–≤–¥–æ–Ω–∏–º (RU) (Enter –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è): ").strip()
            if not alias_ru: break
            term_data['aliases'].append({"ru": alias_ru})
    if "characteristics" in term_data: # –ü–µ—Ä—Å–æ–Ω–∞–∂
        for key, val in term_data["characteristics"].items():
            new_val = input(f"  characteristics.{key} (Enter, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å '{val}'): ").strip()
            if new_val: term_data["characteristics"][key] = new_val
    elif "type" in term_data: # –¢–µ—Ä–º–∏–Ω
        new_type = input(f"  type (Enter, —á—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å '{term_data['type']}'): ").strip()
        if new_type: term_data['type'] = new_type
    system_logger.info("--- –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ ---")
    return term_data

def present_for_confirmation(new_terms: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    if not any(new_terms.values()):
        system_logger.info("\n[TermCollector] –ù–æ–≤—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
        return {}
    term_list = []
    for category, items in new_terms.items():
        for term_id, data in items.items():
            term_list.append({"id": term_id, "category": category, "data": data})
    while True:
        system_logger.info("\n" + "="*40 + "\n  –ù–∞–π–¥–µ–Ω—ã –Ω–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è\n" + "="*40)
        for i, term in enumerate(term_list):
            term_data = term['data']
            jp_name = term_data.get('name', {}).get('jp') or term_data.get('term_jp', 'N/A')
            ru_name = term_data.get('name', {}).get('ru') or term_data.get('term_ru', 'N/A')
            system_logger.info(f"\n--- –¢–µ—Ä–º–∏–Ω #{i+1} ---\n  ID: {term['id']} (–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {term['category']})\n  JP: {jp_name}\n  RU: {ru_name}\n  –û–ø–∏—Å–∞–Ω–∏–µ: {term_data.get('description', 'N/A')}\n  –ö–æ–Ω—Ç–µ–∫—Å—Ç: {term_data.get('context', 'N/A')}")
        system_logger.info("\n" + "-"*40 + "\n  –ö–æ–º–∞–Ω–¥—ã: ok, del <–Ω–æ–º–µ—Ä–∞>, edit <–Ω–æ–º–µ—Ä>, quit\n" + "-"*40)
        try: 
            if os.environ.get('AUTO_ACCEPT_TERMS') == '1':
                command = 'ok'
            else:
                command = input("\n–í–≤–µ–¥–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É: ").strip().lower()
        except EOFError: return None
        if command in ['ok', 'yes', 'y']:
            final_terms = {"characters": {}, "terminology": {}, "expressions": {}}
            for term in term_list:
                if term["category"] in final_terms: final_terms[term["category"]][term["id"]] = term["data"]
            return final_terms
        if command in ['quit', 'exit', 'q']: return None
        parts = command.split()
        if not parts: continue
        action = parts[0]
        try:
            indices = [int(p) - 1 for p in parts[1:]]
            if not all(0 <= i < len(term_list) for i in indices):
                system_logger.warning("–û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π –Ω–æ–º–µ—Ä —Ç–µ—Ä–º–∏–Ω–∞.")
                continue
            if action == 'del':
                for i in sorted(indices, reverse=True): del term_list[i]
                system_logger.info(f"–£–¥–∞–ª–µ–Ω–æ {len(indices)} —Ç–µ—Ä–º–∏–Ω–æ–≤.")
            elif action == 'edit':
                if len(indices) != 1:
                    system_logger.warning("–û—à–∏–±–∫–∞: –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ç–µ—Ä–º–∏–Ω –∑–∞ —Ä–∞–∑.")
                    continue
                idx_to_edit = indices[0]
                term_list[idx_to_edit]["data"] = _edit_term(term_list[idx_to_edit]["data"])
            else: system_logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞: '{action}'")
        except (ValueError, IndexError): system_logger.error("–û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∫–æ–º–∞–Ω–¥—ã.")

def update_glossary_file(new_terms: Dict[str, Any], db_path: Path, source_lang: str = 'ja', target_lang: str = 'ru'):
    if not any(new_terms.values()): return
    system_logger.info(f"\n[TermCollector] –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ SQLite –≥–ª–æ—Å—Å–∞—Ä–∏—è: {db_path}")
    for cat in ["characters", "terminology", "expressions"]:
        for term_id, term_data in new_terms.get(cat, {}).items():
            term_jp = term_data.get("name", {}).get("jp", "")
            term_ru = term_data.get("name", {}).get("ru", "")
            if term_jp and term_ru:
                db.add_term(db_path, term_jp, term_ru, source_lang, target_lang)
    system_logger.info(f"[TermCollector] –ì–ª–æ—Å—Å–∞—Ä–∏–π —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω.")
    if not any(new_terms.values()): return
    system_logger.info(f"\n[TermCollector] –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ SQLite –≥–ª–æ—Å—Å–∞—Ä–∏—è: {db_path}")
    for cat in ["characters", "terminology", "expressions"]:
        for term_id, term_data in new_terms.get(cat, {}).items():
            term_jp = term_data.get("name", {}).get("jp", "")
            term_ru = term_data.get("name", {}).get("ru", "")
            if term_jp and term_ru:
                db.add_term(db_path, term_jp, term_ru, source_lang, target_lang)
    system_logger.info(f"[TermCollector] –ì–ª–æ—Å—Å–∞—Ä–∏–π —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω.")


def collect_terms_from_responses(raw_responses: List[str]) -> Dict[str, Any]:
    """Parse LLM JSON responses and deduplicate terms.
    
    Same JSON parsing + dedup logic as collect_and_deduplicate_terms,
    but accepts raw response strings instead of reading from filesystem.
    """
    unique_terms = {}
    for response_str in raw_responses:
        try:
            # Try to parse as JSON response object first (gemini-cli wraps output)
            try:
                cli_output = json.loads(response_str)
                if isinstance(cli_output, dict) and "response" in cli_output:
                    response_str = cli_output["response"]
            except (json.JSONDecodeError, TypeError):
                pass
            
            match = re.search(r'```json\s*\n(.*?)\s*\n```', response_str, re.DOTALL)
            json_str = match.group(1) if match else response_str
            data = json_repair.loads(json_str)
            if not isinstance(data, dict):
                continue
            for category, items in data.items():
                if not isinstance(items, dict):
                    continue
                for term_id, term_data in items.items():
                    if term_id not in unique_terms:
                        unique_terms[term_id] = {"category": category, "data": term_data}
        except Exception as e:
            system_logger.warning(f"[TermCollector] Failed to parse response: {e}")
    final_structure = {"characters": {}, "terminology": {}, "expressions": {}}
    for term_id, term_info in unique_terms.items():
        cat = term_info["category"]
        if cat in final_structure:
            final_structure[cat][term_id] = term_info["data"]
    return final_structure


def save_approved_terms(terms: Dict[str, Any], glossary_db: Path,
                        source_lang: str = 'ja', target_lang: str = 'ru'):
    """Save approved terms to the series-level glossary database.
    
    Replaces update_glossary_file() ‚Äî writes to series glossary.db instead of state.db.
    """
    count = 0
    for category, items in terms.items():
        for term_id, term_data in items.items():
            term_source = term_data.get('name', {}).get('jp') or \
                          term_data.get('term_jp') or \
                          term_data.get('term_source') or \
                          term_id
            term_target = term_data.get('name', {}).get('ru') or \
                          term_data.get('term_ru') or \
                          term_data.get('term_target', '')
            comment = term_data.get('description') or term_data.get('comment', '')
            if term_source and term_target:
                db.add_term(glossary_db, term_source, term_target,
                           source_lang, target_lang, comment)
                count += 1
    system_logger.info(f"[TermCollector] Saved {count} terms to glossary DB.")


def approve_via_tsv(terms: Dict[str, Any], tsv_path: Path, glossary_db: Path,
                   source_lang: str = 'ja', target_lang: str = 'ru'):
    """Generate TSV ‚Üí wait for user edit ‚Üí import approved terms.
    
    This is the TSV 'approval buffer' workflow.
    """
    # Convert from internal format to list of dicts for glossary_manager
    term_list = []
    for category, items in terms.items():
        for term_id, term_data in items.items():
            term_list.append({
                'term_source': term_data.get('term_jp', term_data.get('term_source', term_id)),
                'term_target': term_data.get('term_ru', term_data.get('term_target', '')),
                'comment': term_data.get('comment', ''),
            })
    
    glossary_manager.generate_approval_tsv(term_list, tsv_path)
    print(f"\nüìù –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª: {tsv_path}")
    print("–£–¥–∞–ª–∏—Ç–µ –Ω–µ–Ω—É–∂–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏, –∏—Å–ø—Ä–∞–≤—å—Ç–µ –ø–µ—Ä–µ–≤–æ–¥—ã.")
    input("–ù–∞–∂–º–∏—Ç–µ Enter –∫–æ–≥–¥–∞ –∑–∞–∫–æ–Ω—á–∏—Ç–µ...")
    count = glossary_manager.import_tsv(glossary_db, tsv_path, source_lang, target_lang)
    print(f"‚úÖ –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {count} —Ç–µ—Ä–º–∏–Ω–æ–≤")
